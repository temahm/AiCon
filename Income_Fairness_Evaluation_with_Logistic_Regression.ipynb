{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwt8pylfkl8o9WtzBx7WTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/temahm/AiCon/blob/main/Income_Fairness_Evaluation_with_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset A: Adult Income (UCI Census Income)**\n",
        "\n",
        "Use case: hiring / income proxy fairness\n",
        "Sensitive attributes: sex, race\n",
        "\n",
        "Why good: no licensing issues"
      ],
      "metadata": {
        "id": "BA7XZp_FBfKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtCdKezGBJ8J"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "df = adult.frame\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5tmVaxRhBcR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset B (Optional / advanced): COMPAS Recidivism (ProPublica)**\n",
        "\n",
        "Use case: justice risk scoring fairness\n",
        "\n",
        "Sensitive attribute: race (and sex)\n",
        "\n",
        "Why good: powerful story\n",
        "\n",
        "Colab load (direct CSV from ProPublica repo)"
      ],
      "metadata": {
        "id": "tlJSKGCZBdZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "L6U--cccC1op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Tools"
      ],
      "metadata": {
        "id": "c-XuLhQJDFDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install fairlearn scikit-learn pandas numpy matplotlib"
      ],
      "metadata": {
        "id": "iEP7bGgIDHEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also XGBoost for comparing Results"
      ],
      "metadata": {
        "id": "VrHUYgdtDKrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install xgboost"
      ],
      "metadata": {
        "id": "otRtXysRDVEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OP41IbxTc-XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "F3EcWLmhdA8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process Adult data set (fast and correct)"
      ],
      "metadata": {
        "id": "jAQrZ4lqDn1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Clean missing values represented as '?'\n",
        "df = df.replace(\"?\", np.nan).dropna()\n",
        "\n",
        "# Define target y (binary)\n",
        "y = (df[\"class\"] == \">50K\").astype(int)\n",
        "\n",
        "# Sensitive features (kept separately for fairness evaluation)\n",
        "A_sex = df[\"sex\"]\n",
        "A_race = df[\"race\"]\n",
        "\n",
        "# Features X: drop target + sensitive columns (you can keep sensitive columns OUT of training)\n",
        "X = df.drop(columns=[\"class\", \"sex\", \"race\"])\n",
        "X = pd.get_dummies(X, drop_first=True)  # one-hot encode categoricals\n"
      ],
      "metadata": {
        "id": "qg4jtqrMDswe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Test data split"
      ],
      "metadata": {
        "id": "TesLjKLTD7TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test, Asex_train, Asex_test = train_test_split(\n",
        "    X, y, A_sex, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "iNJfVo28EDhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) “Right baseline model” line (Logistic Regression)"
      ],
      "metadata": {
        "id": "4EfL24PSER0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "N9z3i61gEZV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions:"
      ],
      "metadata": {
        "id": "QC35KBedEo0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:,1]"
      ],
      "metadata": {
        "id": "8D8-6GKfEtnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall accuracy might look “good”\n",
        "\n",
        "But error rates differ across groups\n",
        "\n",
        "That difference is a fairness risk\n",
        "\n",
        "METRICS TO COMPUTE:\n",
        "\n",
        "Selection rate: how often the model predicts “positive”\n",
        "\n",
        "False Positive Rate (FPR): unfair harm when someone is incorrectly flagged positive\n",
        "\n",
        "False Negative Rate (FNR): unfair harm when someone is incorrectly denied positive\n",
        "\n",
        "**Code: group metrics with Fairlearn MetricFrame**"
      ],
      "metadata": {
        "id": "tlu4ZMQSE1Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics import MetricFrame, selection_rate\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "def false_positive_rate(y_true, y_hat):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
        "    return fp / (fp + tn)\n",
        "\n",
        "def false_negative_rate(y_true, y_hat):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
        "    return fn / (fn + tp)\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score,\n",
        "    \"selection_rate\": selection_rate,\n",
        "    \"FPR\": false_positive_rate,\n",
        "    \"FNR\": false_negative_rate\n",
        "}\n",
        "\n",
        "mf = MetricFrame(\n",
        "    metrics=metrics,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=Asex_test\n",
        ")\n",
        "\n",
        "mf.by_group\n"
      ],
      "metadata": {
        "id": "5XMGmOWKFSu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disparity: max difference"
      ],
      "metadata": {
        "id": "A-Y5fvyaF-Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mf.difference()"
      ],
      "metadata": {
        "id": "I33et4I2GHOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints one number per metric showing how far apart groups are."
      ],
      "metadata": {
        "id": "4I6dLtqqGJ31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Dataset statement\n",
        "\n",
        "“Adult Income dataset; target is income bracket; sensitive attributes include sex/race; used for fairness benchmarking.”\n",
        "\n",
        "B) Model statement\n",
        "\n",
        "“Baseline logistic regression; chosen for transparency and stable behavior; sensitive attributes excluded from training features.”\n",
        "\n",
        "C) Fairness definition statement\n",
        "\n",
        "“We evaluate fairness by comparing selection rates and error rates across groups.”\n",
        "\n",
        "“Large gaps indicate the model may treat groups differently.”\n",
        "\n",
        "\n",
        "D) Interpretation statement\n",
        "“Disparities can stem from historical patterns in data, feature proxies, and model decision boundaries.”"
      ],
      "metadata": {
        "id": "D-KRCT_rGoAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------ ** Human-in-the-loop demo (quick) ** --------------------\n",
        "\n",
        "This simulates a human review on “borderline cases” and shows improvement."
      ],
      "metadata": {
        "id": "cAe2NKFtHVoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick borderline cases near the threshold"
      ],
      "metadata": {
        "id": "I1I85ywXHs1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "borderline = (y_prob > 0.45) & (y_prob < 0.55)"
      ],
      "metadata": {
        "id": "HNjGkaqaHvOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply a simple “human review policy”\n",
        "\n",
        "Example: if borderline negative but strong indicators, flip to positive:"
      ],
      "metadata": {
        "id": "XbX7fMpOH7EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_h = y_pred.copy()\n",
        "\n",
        "rule = (\n",
        "    borderline &\n",
        "    (y_pred == 0) &\n",
        "    (X_test.get(\"education-num\", pd.Series(0, index=X_test.index)) >= 13) &\n",
        "    (X_test.get(\"hours-per-week\", pd.Series(0, index=X_test.index)) >= 40)\n",
        ")\n",
        "\n",
        "y_pred_h[rule] = 1"
      ],
      "metadata": {
        "id": "74H-TJ_DIQmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "recompute fairness metrics (before vs after)"
      ],
      "metadata": {
        "id": "DbuUs2KnIb-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mf_after = MetricFrame(\n",
        "    metrics=metrics,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred_h,\n",
        "    sensitive_features=Asex_test\n",
        ")\n",
        "\n",
        "print(\"BEFORE (by group):\")\n",
        "display(mf.by_group)\n",
        "print(\"\\nBEFORE (disparity):\")\n",
        "display(mf.difference())\n",
        "\n",
        "print(\"\\nAFTER (by group):\")\n",
        "display(mf_after.by_group)\n",
        "print(\"\\nAFTER (disparity):\")\n",
        "display(mf_after.difference())"
      ],
      "metadata": {
        "id": "bKnJKkuaIdjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost (comparison only)\n",
        "- Model most used in real life by ATS systems.\n",
        "- “better accuracy ≠ more fair.”"
      ],
      "metadata": {
        "id": "bTtFEzgqIw9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=300, max_depth=4, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8, random_state=42, eval_metric=\"logloss\"\n",
        ")\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)"
      ],
      "metadata": {
        "id": "EO77373AKLUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now running MetricFrame on y_pred_xgb"
      ],
      "metadata": {
        "id": "0AaTvfi4KXCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics import MetricFrame\n",
        "\n",
        "mf_xgb = MetricFrame(\n",
        "    metrics=metrics,              # same metrics dictionary\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred_xgb,\n",
        "    sensitive_features=Asex_test  # or A_race_test if you're using race\n",
        ")"
      ],
      "metadata": {
        "id": "bW-HFm0eLlyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting a clean scorecard"
      ],
      "metadata": {
        "id": "dOcB3o9BMLNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorecard = mf.by_group.copy()\n",
        "scorecard.loc[\"DISPARITY (max-min)\"] = mf.difference()\n",
        "scorecard"
      ],
      "metadata": {
        "id": "Cbj0KVOeMbXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save scv"
      ],
      "metadata": {
        "id": "_Ccrbr67Mj7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorecard.to_csv(\"AII_scorecard_adult_sex.csv\")"
      ],
      "metadata": {
        "id": "X-1-lwlOMlpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group-Level Results"
      ],
      "metadata": {
        "id": "LCYl6jPnMn19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"XGBoost — Metrics by Group:\")\n",
        "display(mf_xgb.by_group)"
      ],
      "metadata": {
        "id": "sLATmjuxM195"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disparity (max difference across groups)"
      ],
      "metadata": {
        "id": "2gQJbS2OM4D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"XGBoost — Disparity:\")\n",
        "display(mf_xgb.difference())"
      ],
      "metadata": {
        "id": "oxbgC-cVNE2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic vs XGBoost"
      ],
      "metadata": {
        "id": "IUtihcs6NNJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logistic Regression Disparity:\")\n",
        "display(mf.difference())\n",
        "\n",
        "print(\"\\nXGBoost Disparity:\")\n",
        "display(mf_xgb.difference())"
      ],
      "metadata": {
        "id": "v_1GtohoNZlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# “Although XGBoost may improve predictive accuracy, fairness disparities across groups may increase or persist. Higher accuracy does not guarantee equitable outcomes.”\n",
        "\n",
        "Higher predictive accuracy does not mean equal error distribution across groups.\n",
        "\n",
        "XGBoost optimizes global accuracy. Fairness depends on how errors are distributed across subpopulations.\n",
        "\n",
        "Higher predictive accuracy does not mean equal error distribution across groups.\n",
        "\n",
        "XGBoost optimizes global accuracy. Fairness depends on how errors are distributed across subpopulations.\n",
        "\n",
        "XGBoost:\n",
        "Captures nonlinear relationships\n",
        "\n",
        "Detects complex feature interactions\n",
        "\n",
        "Fits fine-grained decision boundaries\n",
        "\n",
        "Minimizes total loss aggressively\n",
        "\n",
        "Logistic regression:\n",
        "\n",
        "Assumes linear relationships\n",
        "\n",
        "Has a single global decision boundary\n",
        "\n",
        "Is less flexible\n",
        "\n",
        "**So XGBoost typically finds patterns logistic regression cannot.**\n",
        "\n",
        "Accuracy is an aggregate metric\n",
        "\n",
        "Accuracy = (Correct predictions) / (Total predictions)\n",
        "\n",
        "It does NOT tell you:\n",
        "\n",
        "Who is being misclassified\n",
        "\n",
        "Which group has higher false positives\n",
        "\n",
        "Which group has higher false negatives\n",
        "\n",
        "Two models can have:\n",
        "\n",
        "Same accuracy\n",
        "\n",
        "Very different group-level errors\n",
        "\n",
        "Or:\n",
        "\n",
        "Higher accuracy\n",
        "\n",
        "\n",
        "**Worse disparity\n",
        "Why XGBoost can amplify disparity**\n",
        "\n",
        "XGBoost builds trees that:\n",
        "Partition data into increasingly specific regions\n",
        "Exploit subtle correlations\n",
        "If features correlate with sensitive attributes (even indirectly), the model may:\n",
        "Learn proxies for protected characteristics\n",
        "Create decision boundaries that disproportionately affect one group\n",
        "\n",
        "Example:\n",
        "Education, zip code, work history, income — all can act as proxies.\n",
        "\n",
        "The more powerful the model, the more precisely it can exploit those patterns.\n",
        "\n",
        "That increases accuracy.\n",
        "But it may also increase disparity."
      ],
      "metadata": {
        "id": "37921Y76No6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why XGBoost can amplify disparity\n",
        "\n",
        "XGBoost builds trees that:\n",
        "\n",
        "Partition data into increasingly specific regions\n",
        "\n",
        "Exploit subtle correlations\n",
        "\n",
        "If features correlate with sensitive attributes (even indirectly), the model may:\n",
        "\n",
        "Learn proxies for protected characteristics\n",
        "\n",
        "Create decision boundaries that disproportionately affect one group\n",
        "\n",
        "Example:\n",
        "Education, zip code, work history, income — all can act as proxies.\n",
        "\n",
        "The more powerful the model, the more precisely it can exploit those patterns.\n",
        "\n",
        "That increases accuracy.\n",
        "But it may also increase disparity."
      ],
      "metadata": {
        "id": "Ml1ja4j3XSiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Key Insight\n",
        "\n",
        "More predictive power = more ability to learn structural inequalities in the data.\n",
        "\n",
        "If historical bias exists in data:\n",
        "\n",
        "A more powerful model will reproduce it more efficiently.\n",
        "\n",
        "Accuracy measures fit to historical reality.\n",
        "\n",
        "Fairness evaluates whether that reality should be reproduced.\n",
        "That’s the core philosophical tension."
      ],
      "metadata": {
        "id": "FVAxkPoZXv7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Accuracy measures how well the model predicts the past. Fairness measures how evenly the model distributes its mistakes. A more powerful model can predict the past better — including past inequities.**"
      ],
      "metadata": {
        "id": "_miEnnIiX6ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparison = pd.DataFrame({\n",
        "    \"Logistic_Disparity\": mf.difference(),\n",
        "    \"XGBoost_Disparity\": mf_xgb.difference()\n",
        "})\n",
        "\n",
        "comparison"
      ],
      "metadata": {
        "id": "V05hwtmrNojv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "graph..."
      ],
      "metadata": {
        "id": "R-oajALsO8ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from fairlearn.metrics import MetricFrame, false_positive_rate\n",
        "\n",
        "# Define metric dictionary (only FPR for clean visualization)\n",
        "metrics_fpr = {\"FPR\": false_positive_rate}\n",
        "\n",
        "# Logistic model\n",
        "mf_log = MetricFrame(\n",
        "    metrics=metrics_fpr,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=Asex_test\n",
        ")\n",
        "\n",
        "# XGBoost model\n",
        "mf_xgb = MetricFrame(\n",
        "    metrics=metrics_fpr,\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred_xgb,\n",
        "    sensitive_features=Asex_test\n",
        ")\n",
        "\n",
        "# Convert to dataframe\n",
        "df_plot = pd.DataFrame({\n",
        "    \"Logistic Regression\": mf_log.by_group[\"FPR\"],\n",
        "    \"XGBoost\": mf_xgb.by_group[\"FPR\"]\n",
        "})\n",
        "\n",
        "df_plot"
      ],
      "metadata": {
        "id": "T5srpZejO5jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot.plot(kind=\"bar\", figsize=(8,5))\n",
        "plt.title(\"False Positive Rate by Group\")\n",
        "plt.ylabel(\"False Positive Rate\")\n",
        "plt.xlabel(\"Group\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J_3Anv17O_KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model more likely to incorrectly label individuals from this group as high risk. That is a measurable bias.”"
      ],
      "metadata": {
        "id": "ysXQJ1aTPdL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing disparity: The maximum difference in FPR between groups is X%."
      ],
      "metadata": {
        "id": "8Dgs_aAJPpw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logistic FPR Disparity:\", mf_log.difference()[\"FPR\"])\n",
        "print(\"XGBoost FPR Disparity:\", mf_xgb.difference()[\"FPR\"])"
      ],
      "metadata": {
        "id": "_dTw-uZHPq0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before vs After Human-in-the-Loop\n",
        "\n",
        "Using Adult dataset\n",
        "\n",
        "(You can swap to race by replacing Asex_test with Arace_test)"
      ],
      "metadata": {
        "id": "IivFT4rXSwp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get probabilities + baseline predictions"
      ],
      "metadata": {
        "id": "onfzT_MYTLuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline (Logistic) predictions\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob >= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "XN7G-Ux9S5eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a Human-in-the-Loop (HITL) “borderline review” policy**\n",
        "\n",
        "We will:\n",
        "\n",
        "Identify borderline cases near the decision boundary (uncertain)\n",
        "\n",
        "Apply a consistent “human review guideline” to a small subset\n",
        "\n",
        "Recompute fairness metrics"
      ],
      "metadata": {
        "id": "A-ho0pbLTSBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "threshold = 0.5\n",
        "band_low, band_high = 0.45, 0.55\n",
        "borderline = (y_prob >= band_low) & (y_prob <= band_high)"
      ],
      "metadata": {
        "id": "nfVY-QUCTha_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Human review rule**\n",
        "\n",
        "For borderline cases predicted negative, flip to positive if “strong indicators” exist.\n",
        "\n",
        "Adult dataset features typically include education-num, hours-per-week, capital-gain (after one-hot).\n",
        "\n",
        "Important: If your one-hot encoding changed column names, you may need to adjust feature access"
      ],
      "metadata": {
        "id": "X3M4qWc7TlrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "y_pred_h = y_pred.copy()\n",
        "\n",
        "# Safe gets (won't crash if column missing)\n",
        "edu = X_test[\"education-num\"] if \"education-num\" in X_test.columns else pd.Series(0, index=X_test.index)\n",
        "hrs = X_test[\"hours-per-week\"] if \"hours-per-week\" in X_test.columns else pd.Series(0, index=X_test.index)\n",
        "cap = X_test[\"capital-gain\"] if \"capital-gain\" in X_test.columns else pd.Series(0, index=X_test.index)\n",
        "\n",
        "human_rule = (\n",
        "    borderline &\n",
        "    (y_pred == 0) &\n",
        "    (edu >= 13) &\n",
        "    (hrs >= 40) &\n",
        "    (cap > 0)\n",
        ")\n",
        "\n",
        "y_pred_h[human_rule] = 1\n",
        "\n",
        "print(\"Borderline cases:\", borderline.sum())\n",
        "print(\"Human overrides applied:\", human_rule.sum())"
      ],
      "metadata": {
        "id": "5MdSOu9FT91u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute fairness metrics Before vs After (FPR by group)"
      ],
      "metadata": {
        "id": "e7vCHyJhUIUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics import MetricFrame, false_positive_rate\n",
        "\n",
        "mf_before = MetricFrame(\n",
        "    metrics={\"FPR\": false_positive_rate},\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred,\n",
        "    sensitive_features=Asex_test\n",
        ")\n",
        "\n",
        "mf_after = MetricFrame(\n",
        "    metrics={\"FPR\": false_positive_rate},\n",
        "    y_true=y_test,\n",
        "    y_pred=y_pred_h,\n",
        "    sensitive_features=Asex_test\n",
        ")\n",
        "\n",
        "print(\"FPR by group (BEFORE):\")\n",
        "display(mf_before.by_group)\n",
        "\n",
        "print(\"FPR by group (AFTER):\")\n",
        "display(mf_after.by_group)\n",
        "\n",
        "print(\"FPR disparity BEFORE:\", mf_before.difference()[\"FPR\"])\n",
        "print(\"FPR disparity AFTER :\", mf_after.difference()[\"FPR\"])"
      ],
      "metadata": {
        "id": "FK_svSnnUJno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Chart: Before vs After HITL Adjustment"
      ],
      "metadata": {
        "id": "yfb9GFDGUP_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "df_hitl_plot = pd.DataFrame({\n",
        "    \"Before (Baseline)\": mf_before.by_group[\"FPR\"],\n",
        "    \"After (HITL Review)\": mf_after.by_group[\"FPR\"]\n",
        "})\n",
        "\n",
        "df_hitl_plot.plot(kind=\"bar\", figsize=(8,5))\n",
        "plt.title(\"False Positive Rate by Group — Before vs After Human-in-the-Loop Review\")\n",
        "plt.ylabel(\"False Positive Rate\")\n",
        "plt.xlabel(\"Group\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1qi1322ZUYWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we measure fairness as differences in error rates across groups. Then we apply a targeted human-in-the-loop review only to borderline, uncertain cases, using transparent guidelines. This reduces disparity while keeping most of the model automated. In AII, these human decisions are logged, auditable, and policy-driven—not arbitrary."
      ],
      "metadata": {
        "id": "Cv4jWxmxUnxl"
      }
    }
  ]
}