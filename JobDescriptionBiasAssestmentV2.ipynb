{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPiVmryiBVMR+toLK+Fi/60",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/temahm/AiCon/blob/main/JobDescriptionBiasAssestmentV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs and imports"
      ],
      "metadata": {
        "id": "xI7OACfAcExd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbmYNYYhb6eb"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U google-genai\n",
        "import pandas as pd\n",
        "import re, json\n",
        "from google.colab import userdata\n",
        "from google import genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias keyword dictionary"
      ],
      "metadata": {
        "id": "TeYLbqWScKv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bias_keywords = {}\n",
        "\n",
        "bias_keywords['Gender Bias'] = [\n",
        "    'he', 'she', 'gentleman', 'lady', 'his', 'her', 'manpower', 'salesman', 'foreman',\n",
        "    'waitress', 'actress', 'mankind', 'rockstar', 'ninja', 'dominant'\n",
        "]\n",
        "\n",
        "bias_keywords['Age Bias'] = [\n",
        "    'young', 'energetic', 'recent graduate', 'digital native', 'seasoned', 'mature',\n",
        "    'senior', 'youthful', 'fresh talent'\n",
        "]\n",
        "\n",
        "bias_keywords['Racial/Ethnic Bias'] = [\n",
        "    'native speaker', 'cultural fit'\n",
        "]\n",
        "\n",
        "bias_keywords['Disability/ADA Bias'] = [\n",
        "    'able-bodied', 'stand', 'walk', 'lift heavy objects', 'physically demanding',\n",
        "    'independent mobility', 'hearing', 'sight'\n",
        "]\n",
        "\n",
        "bias_keywords['Socioeconomic Bias'] = [\n",
        "    'prestigious university', 'elite background', 'unpaid internship', 'car ownership'\n",
        "]\n",
        "\n",
        "bias_keywords['Exclusionary Language'] = [\n",
        "    'must have', 'only', 'everyone knows', 'we expect', 'exclusive'\n",
        "]\n",
        "\n",
        "bias_keywords['Regulatory Risk'] = [\n",
        "    'under 30', 'healthy', 'married', 'single', 'family status', 'nationality',\n",
        "    'religion', 'religions', 'criminal record', 'background checks'\n",
        "]\n",
        "\n",
        "print(\"Bias keywords dictionary initialized.\")"
      ],
      "metadata": {
        "id": "06n-UwCbcP6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste job description input"
      ],
      "metadata": {
        "id": "pg0RJKALcSmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Paste the job description. Press Enter on an empty line TWICE to finish.\\n\")\n",
        "\n",
        "lines = []\n",
        "empty_count = 0\n",
        "\n",
        "while True:\n",
        "    line = input()\n",
        "    if line.strip() == \"\":\n",
        "        empty_count += 1\n",
        "        if empty_count >= 2:\n",
        "            break\n",
        "    else:\n",
        "        empty_count = 0\n",
        "        lines.append(line)\n",
        "\n",
        "job_description_text = \"\\n\".join(lines).strip()\n",
        "\n",
        "print(\"\\nJob description received.\")\n",
        "print(\"Characters:\", len(job_description_text))\n",
        "print(\"Preview:\\n\", job_description_text[:500], \"...\\n\")"
      ],
      "metadata": {
        "id": "GZ3fgfVGcXL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini client setup + model auto-pick"
      ],
      "metadata": {
        "id": "p0thZPgTcamD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "if not API_KEY:\n",
        "    raise RuntimeError(\"Add your Gemini API key in Colab Secrets as GEMINI_API_KEY\")\n",
        "\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "def pick_generate_content_model(client):\n",
        "    # Prefer a “flash” model for speed/cost, but accept anything that supports generateContent.\n",
        "    preferred = [\n",
        "        \"models/gemini-2.5-flash\",\n",
        "        \"models/gemini-2.0-flash\",\n",
        "        \"models/gemini-1.5-flash\",\n",
        "        \"models/gemini-1.5-pro\",\n",
        "    ]\n",
        "\n",
        "    available = []\n",
        "    for m in client.models.list():\n",
        "        name = getattr(m, \"name\", None)\n",
        "        methods = getattr(m, \"supported_actions\", None) or getattr(m, \"supported_methods\", None)\n",
        "        methods_str = \" \".join(methods) if isinstance(methods, (list, tuple)) else str(methods)\n",
        "        if name and (\"generateContent\" in methods_str or \"generate_content\" in methods_str):\n",
        "            available.append(name)\n",
        "\n",
        "    # pick preferred if available\n",
        "    for p in preferred:\n",
        "        if p in available:\n",
        "            return p\n",
        "\n",
        "    # otherwise return the first available\n",
        "    return available[0] if available else None\n",
        "\n",
        "MODEL_NAME = pick_generate_content_model(client)\n",
        "if not MODEL_NAME:\n",
        "    raise RuntimeError(\"No models available for generateContent with this API key.\")\n",
        "\n",
        "print(\"Using model:\", MODEL_NAME)\n",
        "\n",
        "def llm(prompt: str) -> str:\n",
        "    resp = client.models.generate_content(\n",
        "        model=MODEL_NAME,\n",
        "        contents=prompt\n",
        "    )\n",
        "    return (resp.text or \"\").strip()"
      ],
      "metadata": {
        "id": "YuvPdp_BcaCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Robust keyword matching"
      ],
      "metadata": {
        "id": "lYMmbjencigR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_keyword_pattern(keyword: str) -> re.Pattern:\n",
        "    kw = keyword.strip()\n",
        "    tokens = [re.escape(t) for t in kw.split()]\n",
        "    if len(tokens) == 1:\n",
        "        pattern = r'(?<!\\w)' + tokens[0] + r'(?!\\w)'\n",
        "    else:\n",
        "        pattern = r'(?<!\\w)' + r'\\s+'.join(tokens) + r'(?!\\w)'\n",
        "    return re.compile(pattern, flags=re.IGNORECASE)\n",
        "\n",
        "def get_context(text: str, start: int, end: int, window: int = 140) -> str:\n",
        "    cs = max(0, start - window)\n",
        "    ce = min(len(text), end + window)\n",
        "    return text[cs:ce].strip()\n",
        "\n",
        "raw_hits = []\n",
        "for category, keywords in bias_keywords.items():\n",
        "    for keyword in keywords:\n",
        "        rx = make_keyword_pattern(keyword)\n",
        "        for m in rx.finditer(job_description_text):\n",
        "            raw_hits.append({\n",
        "                \"source\": \"keyword_scan\",\n",
        "                \"category\": category,\n",
        "                \"keyword\": keyword,\n",
        "                \"phrase\": m.group(0),\n",
        "                \"start\": m.start(),\n",
        "                \"end\": m.end(),\n",
        "                \"context\": get_context(job_description_text, m.start(), m.end()),\n",
        "            })\n",
        "\n",
        "print(\"Raw keyword hits found:\", len(raw_hits))\n",
        "if raw_hits:\n",
        "    for h in raw_hits[:10]:\n",
        "        print(f\"- [{h['category']}] '{h['phrase']}' (keyword='{h['keyword']}')\")"
      ],
      "metadata": {
        "id": "EApw7Wosc35B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM “open discovery” (find issues even without keywords)"
      ],
      "metadata": {
        "id": "mABHj1OPc7BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discovery_prompt = f\"\"\"\n",
        "You are auditing a job description for potentially biased, exclusionary, or legally risky language.\n",
        "\n",
        "Task:\n",
        "1) Identify any phrases that could discourage protected groups or raise compliance concerns (gender, age, race/ethnicity, disability/ADA, family status, nationality, religion, etc.).\n",
        "2) Also identify overly exclusionary requirements (e.g., \"only\", \"must have X years\", \"native speaker\", etc.) when phrased in a way that could be unfair.\n",
        "3) Return results as JSON ONLY in the exact format below.\n",
        "\n",
        "Return JSON format:\n",
        "{{\n",
        "  \"findings\": [\n",
        "    {{\n",
        "      \"phrase\": \"...exact phrase from the text...\",\n",
        "      \"category\": \"Gender Bias | Age Bias | Racial/Ethnic Bias | Disability/ADA Bias | Socioeconomic Bias | Exclusionary Language | Regulatory Risk | Other\",\n",
        "      \"severity\": 1,\n",
        "      \"reason\": \"...short reason...\",\n",
        "      \"suggested_rewrite\": \"...neutral alternative...\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Job description:\n",
        "\\\"\\\"\\\"{job_description_text}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "discovery_text = llm(discovery_prompt)\n",
        "print(discovery_text[:1200], \"...\\n\")"
      ],
      "metadata": {
        "id": "CPW41OOMdACp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse discovery JSON safely + merge with keyword hits"
      ],
      "metadata": {
        "id": "H8z939JAdD8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_json(text: str):\n",
        "    # Best-effort: find first {...} block\n",
        "    start = text.find(\"{\")\n",
        "    end = text.rfind(\"}\")\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        return None\n",
        "    try:\n",
        "        return json.loads(text[start:end+1])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "discovery = extract_json(discovery_text)\n",
        "llm_findings = discovery.get(\"findings\", []) if discovery else []\n",
        "\n",
        "print(\"LLM discovery findings:\", len(llm_findings))\n",
        "\n",
        "merged = []\n",
        "\n",
        "# Add keyword hits as \"candidates\" (will validate with LLM next)\n",
        "for h in raw_hits:\n",
        "    merged.append({\n",
        "        \"source\": h[\"source\"],\n",
        "        \"category_hint\": h[\"category\"],\n",
        "        \"phrase\": h[\"phrase\"],\n",
        "        \"context\": h[\"context\"],\n",
        "        \"severity\": None,\n",
        "        \"reason\": None,\n",
        "        \"suggested_rewrite\": None,\n",
        "        \"llm_validated\": None\n",
        "    })\n",
        "\n",
        "# Add discovery findings directly (still validate later for consistency)\n",
        "for f in llm_findings:\n",
        "    merged.append({\n",
        "        \"source\": \"llm_discovery\",\n",
        "        \"category_hint\": f.get(\"category\", \"Other\"),\n",
        "        \"phrase\": f.get(\"phrase\", \"\").strip(),\n",
        "        \"context\": None,\n",
        "        \"severity\": f.get(\"severity\", None),\n",
        "        \"reason\": f.get(\"reason\", None),\n",
        "        \"suggested_rewrite\": f.get(\"suggested_rewrite\", None),\n",
        "        \"llm_validated\": None\n",
        "    })\n",
        "\n",
        "# Remove empties and de-duplicate by phrase\n",
        "cleaned = []\n",
        "seen = set()\n",
        "for item in merged:\n",
        "    phr = (item[\"phrase\"] or \"\").strip()\n",
        "    if not phr:\n",
        "        continue\n",
        "    key = phr.lower()\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    cleaned.append(item)\n",
        "\n",
        "print(\"Merged unique candidates:\", len(cleaned))"
      ],
      "metadata": {
        "id": "j0vRIXHOdH0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validate each candidate with the LLM (YES/NO + final fields)"
      ],
      "metadata": {
        "id": "CoqXzOvVdKbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_one(phrase, category_hint, context_hint):\n",
        "    # If we don't have context, ask model to find it or assess generally\n",
        "    context_text = context_hint or job_description_text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are validating whether a flagged phrase is truly problematic in context.\n",
        "\n",
        "Return JSON ONLY:\n",
        "{{\n",
        "  \"is_issue\": true,\n",
        "  \"category\": \"Gender Bias | Age Bias | Racial/Ethnic Bias | Disability/ADA Bias | Socioeconomic Bias | Exclusionary Language | Regulatory Risk | Other\",\n",
        "  \"severity\": 1,\n",
        "  \"reason\": \"short reason\",\n",
        "  \"suggested_rewrite\": \"neutral rewrite\"\n",
        "}}\n",
        "\n",
        "Phrase: \"{phrase}\"\n",
        "Category hint: \"{category_hint}\"\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"{context_text}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    text = llm(prompt)\n",
        "    data = extract_json(text)\n",
        "    return data, text\n",
        "\n",
        "validated = []\n",
        "for item in cleaned:\n",
        "    data, raw = validate_one(item[\"phrase\"], item[\"category_hint\"], item[\"context\"])\n",
        "    if not data:\n",
        "        # If parsing fails, mark unvalidated\n",
        "        item[\"llm_validated\"] = False\n",
        "        validated.append(item)\n",
        "        continue\n",
        "\n",
        "    if data.get(\"is_issue\") is True:\n",
        "        item[\"llm_validated\"] = True\n",
        "        item[\"category\"] = data.get(\"category\", item[\"category_hint\"])\n",
        "        item[\"severity\"] = data.get(\"severity\", item[\"severity\"])\n",
        "        item[\"reason\"] = data.get(\"reason\", item[\"reason\"])\n",
        "        item[\"suggested_rewrite\"] = data.get(\"suggested_rewrite\", item[\"suggested_rewrite\"])\n",
        "        validated.append(item)\n",
        "\n",
        "print(\"Validated issues kept:\", len(validated))"
      ],
      "metadata": {
        "id": "6rlH3zuodScQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias Risk Score (0–100) + rubric\n",
        "This score is explainable and stable.\n",
        "Rubric (simple, transparent):\n",
        "Each validated issue adds points based on severity:\n",
        "severity 1 → +4\n",
        "severity 2 → +8\n",
        "severity 3 → +12\n",
        "severity 4 → +16\n",
        "severity 5 → +20\n",
        "Category multipliers:\n",
        "Regulatory Risk, Disability/ADA → ×1.25\n",
        "Racial/Ethnic Bias, Age Bias, Gender Bias → ×1.15\n",
        "Exclusionary Language, Socioeconomic Bias → ×1.0\n",
        "Other → ×0.8\n",
        "Cap at 100."
      ],
      "metadata": {
        "id": "ieZp7TPsdTdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def severity_points(sev):\n",
        "    try:\n",
        "        sev = int(sev)\n",
        "    except:\n",
        "        sev = 1\n",
        "    return {1:4, 2:8, 3:12, 4:16, 5:20}.get(sev, 4)\n",
        "\n",
        "category_multiplier = {\n",
        "    \"Regulatory Risk\": 1.25,\n",
        "    \"Disability/ADA Bias\": 1.25,\n",
        "    \"Racial/Ethnic Bias\": 1.15,\n",
        "    \"Age Bias\": 1.15,\n",
        "    \"Gender Bias\": 1.15,\n",
        "    \"Exclusionary Language\": 1.0,\n",
        "    \"Socioeconomic Bias\": 1.0,\n",
        "    \"Other\": 0.8,\n",
        "}\n",
        "\n",
        "score_raw = 0.0\n",
        "for it in validated:\n",
        "    cat = it.get(\"category\", it.get(\"category_hint\", \"Other\"))\n",
        "    sev = it.get(\"severity\", 1)\n",
        "    pts = severity_points(sev)\n",
        "    score_raw += pts * category_multiplier.get(cat, 1.0)\n",
        "\n",
        "bias_score = min(100, round(score_raw, 1))\n",
        "\n",
        "def score_band(s):\n",
        "    if s <= 10: return \"Low\"\n",
        "    if s <= 30: return \"Moderate\"\n",
        "    if s <= 60: return \"High\"\n",
        "    return \"Very High\"\n",
        "\n",
        "print(\"Bias Risk Score (0–100):\", bias_score)\n",
        "print(\"Risk Level:\", score_band(bias_score))\n",
        "print(\"Validated issue count:\", len(validated))"
      ],
      "metadata": {
        "id": "YhGj2dCVdc-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final report table + grouped summary"
      ],
      "metadata": {
        "id": "uvGwLGuPdgdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(validated)\n",
        "\n",
        "# normalize category field\n",
        "if \"category\" not in df.columns:\n",
        "    df[\"category\"] = df[\"category_hint\"]\n",
        "\n",
        "# order columns nicely\n",
        "cols = [\"category\", \"severity\", \"phrase\", \"reason\", \"suggested_rewrite\", \"source\"]\n",
        "df = df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "print(\"\\n=== Summary by category ===\")\n",
        "if len(df) == 0:\n",
        "    print(\"No validated issues found.\")\n",
        "else:\n",
        "    display(df.sort_values([\"category\", \"severity\"], ascending=[True, False]))\n",
        "\n",
        "    summary = df.groupby(\"category\").size().sort_values(ascending=False)\n",
        "    print(summary)\n",
        "\n",
        "print(\"\\n=== Suggested rewrites (top 15) ===\")\n",
        "if len(df) > 0:\n",
        "    for i, row in df.head(15).iterrows():\n",
        "        print(f\"- '{row['phrase']}' → {row.get('suggested_rewrite','')}\")"
      ],
      "metadata": {
        "id": "-7eLPXVhdlQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}